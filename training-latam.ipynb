{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model retraining using MLM task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_GPU = 2 # Available GPU with 0% usage\n",
    "HUGGINGFACE_MODEL = \"dccuchile/bert-base-spanish-wwm-cased\" # HuggingFace checkpoint model to train\n",
    "MODEL_SAVE_PATH = \"./output/latam-old-spanish-beto-cased.pt\" # Path to save the model\n",
    "\n",
    "MASK_PROB = 0.15 # Probability of masking within a text\n",
    "LR = 2e-5 # Learning rate for Adam\n",
    "EPOCHS = 5 # Number of epochs to train\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "MAX_TOKENIZER_LENGTH = 512 # Maximum length of the texts in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/historynlp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/historynlp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{AVAILABLE_GPU}\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "tf_device=f'/gpu:{AVAILABLE_GPU}'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.optim import Adam\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-trained LM for MLM and it's tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tokenizer and the model, using the Model's `transformers` ID from Hugging Face. It's important to consider the possible differences between _cased_ and _uncased_ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL)\n",
    "model = AutoModelForMaskedLM.from_pretrained(HUGGINGFACE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 1, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_TOKEN = tokenizer.cls_token_id\n",
    "SEP_TOKEN = tokenizer.sep_token_id\n",
    "PAD_TOKEN = tokenizer.pad_token_id\n",
    "MASK_TOKEN = tokenizer.mask_token_id\n",
    "CLS_TOKEN, SEP_TOKEN, PAD_TOKEN, MASK_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and pre-process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, we'll take the training split from the full spanish corpus, after cleaning. Given after the preparation stage:\n",
    "> This corpus is already chunked for no more than 512 tokens, so there will be no chunking required for models that has this maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'text'],\n",
       "    num_rows: 29972\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/old-spanish-corpus-chunked.tsv\", sep=\"\\t\", usecols=[\"text\", \"source\"])\n",
    "df = df[(df.source == \"LatamXIX\")]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agregábase un acontecimiento del exterior: el 4 de Agosto de 1741 la Suecia declaró la guerra al imperio ruso; pero ya el 25 del mismo mes el general ruso Keith, cuyo gobierno fué puesto al corriente de aquella declaración y de todos los planes de la Sue'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10000][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize and Mask the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's possible to start the retraining process of the model for the MLM task. For that, we'll first tokenize the input texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tokenizer_output\n",
    "%%time\n",
    "\n",
    "# All the texts of our dataset are stored in dataset[\"text\"]\n",
    "inputs = tokenizer(dataset[\"text\"], return_tensors='pt', max_length=MAX_TOKENIZER_LENGTH, truncation=True, padding='max_length')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 3.84 s, total: 31.7 s\n",
      "Wall time: 4.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    4,  1032, 12204,  ...,     1,     1,     1],\n",
       "        [    4,  1139,  1009,  ...,     1,     1,     1],\n",
       "        [    4,  1044,  2570,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    4,  2991,  1054,  ...,     1,     1,     1],\n",
       "        [    4,  1015, 29848,  ...,     1,     1,     1],\n",
       "        [    4,  9907,  1019,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparation between the masked sentence and the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Palabra , el 2º Óvalo no parecia sino un panteon en los dias de Todos Santos. Al fin quiso hablar [aunque sin ocupar la tribuna] un Orangutan ; pero el infeliz padecia de una afeccion asmatica cruelisima y no pudo mas que grasnar y tocer largo rato; así es , que despues de haber paseado sobre todos sus camaradas , tiernas y agonizantes miradas, acabó por extender su larga mano y se despidió. La Hormiga abrazada de la Jirafa , y dejando correr un torrente de amargo llanto por sus hermosas , aunque tostadas mejillas , se despidió tambien , pero á la francesa con tres besos en los carrillos y tres abrazos fortísimos. Tanto que la palpitacion de la que hemos hablado , hubo de acabar con su interesante existencia.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] palabra , el [UNK] ó ##valo no pareci ##a sino un pante ##on en los dias de todos santos . al fin quiso hablar [ aunque sin ocupar la tribuna ] un ora ##ng ##uta ##n ; pero el infeliz pad ##ecia de una afec ##cion asma ##tica cruel ##isi ##ma y no pudo mas que gras ##nar y toc ##er largo rato ; así es , que despues de haber pase ##ado sobre todos sus camaradas , tier ##nas y agon ##izantes miradas , acabó por extender su larga mano y se despidió . la horm ##iga abra ##zada de la ji ##ra ##fa , y dejando correr un torrente de amargo llanto por sus hermosas , aunque tos ##tadas mejillas , se despidió tambien , pero á la francesa con tres besos en los carril ##los y tres abrazo ##s fort ##ísimos . tanto que la palp ##ita ##cion de la que hemos hablado , hubo de acabar con su interesante existencia . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([tokenizer.decode(x).replace(' ', '') for x in inputs['input_ids'][100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the `labels` of the inputs are defined as a copy of the `input_ids`, and some percentage of the inputs are masked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < MASK_PROB) * (inputs.input_ids != CLS_TOKEN) * (inputs.input_ids != SEP_TOKEN) * (inputs.input_ids != PAD_TOKEN)\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4,  1032, 12204,  ...,     1,     1,     1],\n",
       "        [    4,  1139,     0,  ...,     1,     1,     1],\n",
       "        [    4,  1044,  2570,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    4,  2991,  1054,  ...,     1,     1,     1],\n",
       "        [    4,  1015, 29848,  ...,     1,     1,     1],\n",
       "        [    4,  9907,  1019,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask_arr.bool()\n",
    "indices = mask.nonzero(as_tuple=False)\n",
    "inputs.input_ids[indices[:, 0], indices[:, 1]] = MASK_TOKEN \n",
    "\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train with Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the processing, and efficiency for training the model, the `inputs` object will be converted to a Dataset object, and there will be defined an optimizer for the training. Also, if there's available GPU, the model will be moved to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldSpanishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings.input_ids.shape[0]\n",
    "\n",
    "dataset = OldSpanishDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.train()\n",
    "optim = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DataParallel(model, device_ids=[1, 2])\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture training_output\n",
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "\n",
    "    print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} [end]\")\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | step 000 Loss: 13.151481628417969 \n",
      "Epoch 0 | step 936 Loss: 0.22948646545410156 [end]\n",
      "Epoch 1 | step 000 Loss: 0.23694846034049988 \n",
      "Epoch 1 | step 936 Loss: 0.1889580637216568 [end]\n",
      "Epoch 2 | step 000 Loss: 0.19381776452064514 \n",
      "Epoch 2 | step 936 Loss: 0.13998401165008545 [end]\n",
      "Epoch 3 | step 000 Loss: 0.18480820953845978 \n",
      "Epoch 3 | step 936 Loss: 0.1526048332452774 [end]\n",
      "Epoch 4 | step 000 Loss: 0.1915740668773651 \n",
      "Epoch 4 | step 936 Loss: 0.16695943474769592 [end]\n",
      "Training completed\n",
      "CPU times: user 1h 15min 12s, sys: 1.95 s, total: 1h 15min 14s\n",
      "Wall time: 1h 15min 13s\n"
     ]
    }
   ],
   "source": [
    "training_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the model into a file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
