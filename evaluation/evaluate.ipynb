{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_GPU = 3\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= f\"{AVAILABLE_GPU}\" # ALWAYS look the one with 0% usage\n",
    "tf_device=f'/gpu:{AVAILABLE_GPU}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/historynlp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.cluster import KMeans, AffinityPropagation\n",
    "import numpy as np\n",
    "from torch.nn.functional import normalize\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "avg = lambda *r : sum(r)/len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOULD_PRINT = False\n",
    "MODEL = 'dccuchile/albert-base-spanish'\n",
    "MODEL_TYPE = 'pure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = MODEL\n",
    "if MODEL_TYPE == \"trained\":\n",
    "    if 'beto' in MODEL:\n",
    "        if 'uncased' in MODEL:\n",
    "            CHECKPOINT = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "        else:\n",
    "            CHECKPOINT = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "    elif 'mbert' in MODEL:\n",
    "        if 'uncased' in MODEL:\n",
    "            CHECKPOINT = 'bert-base-multilingual-uncased'\n",
    "        else:\n",
    "            CHECKPOINT = 'bert-base-multilingual-cased'\n",
    "    else:\n",
    "        CHECKPOINT = 'dccuchile/albert-base-spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "if MODEL_TYPE == \"trained\":\n",
    "    model = torch.load(f\"../output/{MODEL}.pt\").to(device)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>date</th>\n",
       "      <th>identifier</th>\n",
       "      <th>context</th>\n",
       "      <th>indexes_target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abundar</td>\n",
       "      <td>1858</td>\n",
       "      <td>old_corpus_spanish-17804-364</td>\n",
       "      <td>Fueron concluidos en 1570. Hay, por fin, tres ...</td>\n",
       "      <td>27:186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abundar</td>\n",
       "      <td>1858</td>\n",
       "      <td>old_corpus_spanish-314696-1167</td>\n",
       "      <td>Rara vez sonaba en aquellos barrios el importu...</td>\n",
       "      <td>392:1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abundar</td>\n",
       "      <td>1858</td>\n",
       "      <td>old_corpus_spanish-23288-235</td>\n",
       "      <td>—Hablaremos más despacio mañana… Puedes irte t...</td>\n",
       "      <td>111:165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundar</td>\n",
       "      <td>1858</td>\n",
       "      <td>old_corpus_spanish-447700-231</td>\n",
       "      <td>El 30 se notaron violentísimos esfuerzos para ...</td>\n",
       "      <td>85:143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundar</td>\n",
       "      <td>1858</td>\n",
       "      <td>old_corpus_spanish-622360-638</td>\n",
       "      <td>Y siempre que algún forastero de viso se prese...</td>\n",
       "      <td>234:450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>vuestro</td>\n",
       "      <td>2007</td>\n",
       "      <td>modern_corpus_spanish-790506-168</td>\n",
       "      <td>Incluso algunas veces, existen expectativas cu...</td>\n",
       "      <td>69:109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>vuestro</td>\n",
       "      <td>2007</td>\n",
       "      <td>modern_corpus_spanish-868656-234</td>\n",
       "      <td>Por turnos, la nueva señal de tráfico de Gary ...</td>\n",
       "      <td>53:175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>vuestro</td>\n",
       "      <td>2007</td>\n",
       "      <td>modern_corpus_spanish-633875-439</td>\n",
       "      <td>\"En cuanto a táctica política, también se ha m...</td>\n",
       "      <td>190:350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>vuestro</td>\n",
       "      <td>2007</td>\n",
       "      <td>modern_corpus_spanish-797048-553</td>\n",
       "      <td>\"Dije, \"\"si haces, de hecho, 'muéstrame mi bal...</td>\n",
       "      <td>202:322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>vuestro</td>\n",
       "      <td>2007</td>\n",
       "      <td>modern_corpus_spanish-604410-362</td>\n",
       "      <td>No obstante, Occidente no tiene ni el derecho ...</td>\n",
       "      <td>167:327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  date                        identifier  \\\n",
       "0     abundar  1858      old_corpus_spanish-17804-364   \n",
       "1     abundar  1858    old_corpus_spanish-314696-1167   \n",
       "2     abundar  1858      old_corpus_spanish-23288-235   \n",
       "3     abundar  1858     old_corpus_spanish-447700-231   \n",
       "4     abundar  1858     old_corpus_spanish-622360-638   \n",
       "...       ...   ...                               ...   \n",
       "3995  vuestro  2007  modern_corpus_spanish-790506-168   \n",
       "3996  vuestro  2007  modern_corpus_spanish-868656-234   \n",
       "3997  vuestro  2007  modern_corpus_spanish-633875-439   \n",
       "3998  vuestro  2007  modern_corpus_spanish-797048-553   \n",
       "3999  vuestro  2007  modern_corpus_spanish-604410-362   \n",
       "\n",
       "                                                context  \\\n",
       "0     Fueron concluidos en 1570. Hay, por fin, tres ...   \n",
       "1     Rara vez sonaba en aquellos barrios el importu...   \n",
       "2     —Hablaremos más despacio mañana… Puedes irte t...   \n",
       "3     El 30 se notaron violentísimos esfuerzos para ...   \n",
       "4     Y siempre que algún forastero de viso se prese...   \n",
       "...                                                 ...   \n",
       "3995  Incluso algunas veces, existen expectativas cu...   \n",
       "3996  Por turnos, la nueva señal de tráfico de Gary ...   \n",
       "3997  \"En cuanto a táctica política, también se ha m...   \n",
       "3998  \"Dije, \"\"si haces, de hecho, 'muéstrame mi bal...   \n",
       "3999  No obstante, Occidente no tiene ni el derecho ...   \n",
       "\n",
       "     indexes_target_sentence  \n",
       "0                     27:186  \n",
       "1                   392:1045  \n",
       "2                    111:165  \n",
       "3                     85:143  \n",
       "4                    234:450  \n",
       "...                      ...  \n",
       "3995                  69:109  \n",
       "3996                  53:175  \n",
       "3997                 190:350  \n",
       "3998                 202:322  \n",
       "3999                 167:327  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/evaluation/dwug_es_uses.csv', sep='\\t')\n",
    "df_gold = pd.read_csv('../data/evaluation/dwug_es_judgments.csv', sep='\\t')\n",
    "df_gold['judgment_bin'] = np.where(df_gold['judgment'] <= 2, 1, 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nice(input_ids, index, index_end=None, pad_token=None):\n",
    "    if pad_token is not None:\n",
    "        input_ids = [token for token in input_ids if token != pad_token]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    if index_end is None:\n",
    "        tokens[index] = '\\033[94m' + tokens[index] + '\\033[0m'\n",
    "    else:\n",
    "        tokens[index] = '\\033[94m' + tokens[index]\n",
    "        tokens[index_end] = tokens[index_end] + '\\033[0m'\n",
    "    print(' '.join(tokens))\n",
    "\n",
    "def generate_substrings(word):\n",
    "    substrings = []\n",
    "    for i in range(len(word), 0, -1):\n",
    "        substrings.append(word[:i])\n",
    "    return substrings[1:-1]\n",
    "\n",
    "def find_sub_list(sl,l): # not used because some examples have no exact coincidence\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind+1,ind+sll # +1 for the [CLS] token\n",
    "\n",
    "def remove_accents(word):\n",
    "    normalized_word = unicodedata.normalize('NFD', word)\n",
    "    cleaned_word = re.sub(r'[\\u0300-\\u036f]', '', normalized_word)\n",
    "    return cleaned_word\n",
    "\n",
    "def isletter(character):\n",
    "    return re.sub(r'[^а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f]', '', character) != ''\n",
    "\n",
    "def extract_letters(input_string):\n",
    "    # if input_string == \"\":\n",
    "    #     return \"\"\n",
    "    # if not isletter(input_string[0]):\n",
    "    #     input_string = input_string[1:]\n",
    "    # if not isletter(input_string[-1]):\n",
    "    #     input_string = input_string[:-1]\n",
    "\n",
    "    if \"albert\" in MODEL or \"xlm-r\" in MODEL:\n",
    "        punc = re.escape(punctuation)\n",
    "        regex_pattern = r\"[^[\\]а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f'](?=—[¡\" + punc + \"]|$\\\")\"\n",
    "        return re.sub(regex_pattern, '', input_string)\n",
    "    return re.sub(r\"[^[\\]а-яА-Яa-zA-ZÀ-ÿёЁ\\u0300-\\u036f']\", '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_containing_target(sentence, target_word):\n",
    "    index = sentence.find(target_word)\n",
    "    if index == -1:\n",
    "        return None\n",
    "    start_index = sentence.rfind(\" \", 0, index) + 1 if index != 0 else 0\n",
    "    end_index = sentence.find(\" \", index + len(target_word)) if sentence.find(\" \", index + len(target_word)) != -1 else len(sentence)\n",
    "    final_char = final_char = \" \" if target_word.endswith(\" \") else \"\"\n",
    "    return sentence[start_index:end_index].split()[0] + final_char\n",
    "\n",
    "def get_search(example, word, orth=None, print_search=False):\n",
    "    # append the words to search in the example, in the desired ORDER\n",
    "    # 1 - the word (with an ending character), and the word itself\n",
    "    search = [f\"{word} \", f\"{word},\", f\"{word}.\", word]\n",
    "\n",
    "    # 2 - the orthographic form (with an ending character), and the orthographic form itself\n",
    "    if orth and orth != word:\n",
    "        search += [f\"{orth} \", f\"{orth},\", f\"{orth}.\", orth]\n",
    "\n",
    "    # 3 - all substrings of the word (i.e. выходить -> ['выходит', 'выходи', 'выход', 'выхо', 'вых', 'вы'])\n",
    "    search += generate_substrings(word)\n",
    "\n",
    "    # 4 - all substrings of the orthographic form\n",
    "    if orth:\n",
    "        search.extend([i for i in generate_substrings(orth) if i not in search])\n",
    "\n",
    "    # 5 - the word without accents (with an ending character), and the word without accents itself\n",
    "    unicoded_word = remove_accents(word)\n",
    "    if unicoded_word != word:\n",
    "        search += [f\"{unicoded_word} \", f\"{unicoded_word},\", f\"{unicoded_word}.\", unicoded_word]\n",
    "    \n",
    "    # 6 - the orthographic form without accents (with an ending character), and the orthographic form without accents itself\n",
    "    if orth and orth != word:\n",
    "        unicoded_orth = remove_accents(orth)\n",
    "        if unicoded_orth != orth:\n",
    "            search += [f\"{unicoded_orth} \", f\"{unicoded_orth},\", f\"{unicoded_orth}.\", unicoded_orth]\n",
    "\n",
    "    # 7 - all substrings of the word without accents\n",
    "    if unicoded_word != word:\n",
    "        search.extend([i for i in generate_substrings(unicoded_word) if i not in search])\n",
    "\n",
    "    # 8 - all substrings of the orthographic form without accents\n",
    "    if orth and orth != word and unicoded_orth != orth:\n",
    "        search.extend([i for i in generate_substrings(unicoded_orth) if i not in search])\n",
    "\n",
    "    if print_search:\n",
    "        print(f\"Searching for: {search}\")\n",
    "\n",
    "    # FIND the first search-string that is within the example, if any (in upper or lowercase)\n",
    "    for s in search:\n",
    "        search_word = find_word_containing_target(example, s)\n",
    "        if search_word:\n",
    "            break\n",
    "        search_word = find_word_containing_target(example.lower(), s.lower())\n",
    "        if search_word:\n",
    "            index = example.lower().find(search_word)\n",
    "            if index == -1:\n",
    "                # this should never happen\n",
    "                raise Exception(f\"Found '{search_word}' in '{example.lower()}', but then not found...\")\n",
    "            else:\n",
    "                search_word = example[index:index + len(search_word)]\n",
    "            break\n",
    "    else:\n",
    "        search_word = \"\"\n",
    "    return extract_letters(search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▏                             | 29/100 [00:13<00:33,  2.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [00:47<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = dict()\n",
    "word = \"\"\n",
    "word_idx = 0\n",
    "if SHOULD_PRINT: print(df.loc[0, \"lemma\"])\n",
    "\n",
    "with tqdm(total=df['lemma'].nunique()) as pbar:\n",
    "    for index, row in df.iterrows():\n",
    "        if word != \"\" and word != row['lemma']:\n",
    "            if SHOULD_PRINT: print(f\"\\n{row['lemma']}\")\n",
    "            pbar.update(1)\n",
    "            word_idx += 1\n",
    "\n",
    "        should_print = SHOULD_PRINT\n",
    "        word = row['lemma']          # target word\n",
    "        example = row['context']    # usage example of the target word\n",
    "        sentence_indexes = row['indexes_target_sentence']\n",
    "        identifier = row['identifier']\n",
    "\n",
    "        # 1. Get the target word index in the example tokenized\n",
    "        tokens = tokenizer.tokenize(example)\n",
    "\n",
    "        if True:#len(tokens) >= 512:\n",
    "            example = example[int(sentence_indexes.split(':')[0]) : int(sentence_indexes.split(':')[1])]\n",
    "            tokens = tokenizer.tokenize(example)\n",
    "\n",
    "        search_word = get_search(example, word)\n",
    "\n",
    "        if search_word == \"\":\n",
    "            if len(example.split()) == 1:\n",
    "                print(f\"{index}. \\033[91mNot found\\033[0m {word} in '{example}' (taking only word in example)\")\n",
    "                target_index, target_index_end = 1, 1\n",
    "            else:\n",
    "                print(f\"{index}. \\033[91mNot found\\033[0m {word} in '{example}' (taking [CLS] token)\")\n",
    "                target_index, target_index_end = 0, 0\n",
    "        else:\n",
    "            #print(search_word)\n",
    "            #print(example)\n",
    "            search_tokens = tokenizer.tokenize(search_word)\n",
    "            try:\n",
    "                target_index, target_index_end = find_sub_list(search_tokens, tokens)\n",
    "            except:\n",
    "                # this should never happen\n",
    "                raise ValueError(f\"Error unpacking {search_tokens} in {tokens}\")\n",
    "\n",
    "        inputs = tokenizer(example, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "        if should_print:\n",
    "            print_nice(inputs['input_ids'][0], target_index, target_index_end, pad_token=tokenizer.pad_token_id)\n",
    "\n",
    "        # 2. Compute the embedding of the token\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        if MODEL_TYPE == \"trained\": \n",
    "            embedding = outputs.hidden_states[-1][0][target_index]\n",
    "        else:\n",
    "            embedding = outputs.last_hidden_state[0][target_index]\n",
    "\n",
    "        embeddings[identifier] = embedding.cpu()\n",
    "\n",
    "df['embedding'] = embeddings.values() # ids are in the same order\n",
    "assert len(embeddings) == len(df), \"Embeddings and dataframe have different lengths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 62624/62624 [00:36<00:00, 1697.67it/s]\n"
     ]
    }
   ],
   "source": [
    "cdl = []\n",
    "prtl = []\n",
    "\n",
    "for i, row in tqdm(df_gold.iterrows(), total=df_gold.shape[0], position=0, leave=True):\n",
    "    emb1 = normalize(embeddings[row[\"identifier1\"]], p=2, dim=-1).reshape(1, -1)\n",
    "    emb2 = normalize(embeddings[row[\"identifier2\"]], p=2, dim=-1).reshape(1, -1)\n",
    "    cs = cosine_similarity(emb1, emb2)[0][0]\n",
    "    cdl.append(1 - cs)\n",
    "    prtl.append(1 / cs)\n",
    "\n",
    "cd_mean = sum(cdl)/len(cdl)\n",
    "prt_mean = sum(prtl)/len(prtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6372080088632686"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_gold[\"guess\"] = [(min(int(number * 4) + 1, 4)) for number in normalized_csl]\n",
    "#df_gold['guess_bin'] = np.where(df_gold['guess'] <= 2, 1, 0)\n",
    "df_gold[\"guess_bin\"] = [int(cd > cd_mean) for cd in cdl]\n",
    "F1[\"CD\"] = f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')\n",
    "F1[\"CD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Similarity Over Word Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6681654041047085"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gold[\"guess_bin\"] = [int(prt > prt_mean) for prt in prtl]\n",
    "F1[\"PRT\"] = f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')\n",
    "F1[\"PRT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silhouette(tensors, kmeans):\n",
    "    n = kmeans.n_clusters\n",
    "    labels = kmeans.labels_\n",
    "    if n == 1: return 0 # doesn't allow 1-cluster solutions\n",
    "    X = np.array([tensor.flatten().numpy() for tensor in tensors])\n",
    "    return silhouette_score(X, labels=labels, metric='euclidean')\n",
    "\n",
    "def elbow_method(inertia_values):\n",
    "    deltas = []\n",
    "    for i in range(1, len(inertia_values)):\n",
    "        deltas.append(inertia_values[i - 1] - inertia_values[i])\n",
    "    max_curvature_index = deltas.index(max(deltas))\n",
    "    return max_curvature_index + 2\n",
    "\n",
    "def compute_metric(tensors, kmeans, method=\"silhouette\"):\n",
    "    return get_silhouette(tensors, kmeans) if method == \"silhouette\" else 0\n",
    "\n",
    "def KMeans_df(df, metric):\n",
    "    best_score, best_n, inertias = -1, 0, []\n",
    "\n",
    "    min_clusters = 1\n",
    "    max_clusters = int(len(df)**.5)+1\n",
    "\n",
    "    for n in range(min_clusters, max_clusters):\n",
    "        km = KMeans(n_clusters=n, random_state=0, n_init='auto')\n",
    "        km.fit([t.numpy() for t in df['embedding'].tolist()])\n",
    "        df[f'sense_{n}'] = km.labels_\n",
    "        if (metric == 'inertia'):\n",
    "            inertias.append(km.inertia_)\n",
    "        else:\n",
    "            score = compute_metric(df['embedding'], km, metric)\n",
    "            if (metric == 'silhouette') and (score > best_score):\n",
    "                best_score, best_n = score, n\n",
    "            elif (metric == 'compactness') and (score > best_score):\n",
    "                best_score, best_n = score, n\n",
    "    \n",
    "    if metric == 'inertia':\n",
    "        best_n = elbow_method(inertias)\n",
    "        if SHOULD_PRINT:\n",
    "            plt.plot(range(min_clusters, len(inertias) + 1), inertias, marker='o')\n",
    "            plt.xlabel('Number of clusters')\n",
    "            plt.ylabel('Inertia')\n",
    "            plt.title('Inertia (old model)')\n",
    "            plt.show()\n",
    "\n",
    "    df['sense'] = df[f'sense_{best_n}']\n",
    "    df = df.drop(columns=[f'sense_{n}' for n in range(min_clusters, max_clusters)])\n",
    "    \n",
    "    centroids = dict()\n",
    "    intra_deviations = dict()\n",
    "    for label in range(best_n):\n",
    "        cluster_points = [e.numpy() for e in df[df['sense'] == label]['embedding']]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        similarities = cosine_similarity(cluster_points, [centroid])\n",
    "        intra_deviation = np.mean(np.abs(1 - similarities))\n",
    "        centroids[label] = centroid\n",
    "        intra_deviations[label] = intra_deviation\n",
    "\n",
    "    df['centroid'] = df['sense'].map(centroids)\n",
    "    df['intra_deviation'] = df['sense'].map(intra_deviations)\n",
    "\n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"\\n{df.iloc[0]['lemma']}, {best_n} clsuters [max {max_clusters}]\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def AffinityP_df(df, damping=0.5, affinity='precomputed'):\n",
    "    ap = AffinityPropagation(affinity=affinity, damping=damping)\n",
    "    if affinity == 'precomputed': \n",
    "        cd = cosine_distances([t.numpy() for t in df['embedding'].tolist()])\n",
    "    else: \n",
    "        cd = [e.numpy() for e in df['embedding']]\n",
    "    ap.fit(cd)\n",
    "\n",
    "    df['sense'] = ap.labels_\n",
    "    \n",
    "    centroids = dict()\n",
    "    intra_deviations = dict()\n",
    "    for label in np.unique(ap.labels_):\n",
    "        cluster_points = [e.numpy() for e in df[df['sense'] == label]['embedding']]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        similarities = cosine_similarity(cluster_points, [centroid])\n",
    "        intra_deviation = np.mean(np.abs(1 - similarities))\n",
    "        centroids[label] = centroid\n",
    "        intra_deviations[label] = intra_deviation\n",
    "\n",
    "    df['centroid'] = df['sense'].map(centroids)\n",
    "    df['intra_deviation'] = df['sense'].map(intra_deviations)\n",
    "\n",
    "    if SHOULD_PRINT:\n",
    "        print(f\"{df.iloc[0]['lemma']}, {len(ap.cluster_centers_indices_)} clusters\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clustering(df, method, metric):\n",
    "    return KMeans_df(df, metric) if method == \"KM\" else AffinityP_df(df, metric)\n",
    "\n",
    "def cluster_df(df, method=\"AP\", metric=\"silhouette\"):\n",
    "    result_df = pd.DataFrame()\n",
    "    for word, group in tqdm(df.groupby('lemma')):\n",
    "        group_cl = clustering(group, method, metric)\n",
    "        result_df = pd.concat([result_df, group_cl], ignore_index=True)\n",
    "        assert len(group) == len(group_cl), f\"{len(group)} != {len(group_cl)} for word {lemma}\"\n",
    "\n",
    "    return result_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERING_METHOD = \"AP\" # \"KM\" or \"AP\"\n",
    "CLUSTERING_METRIC = 0.975 # For KMeans: \"silhouette\" or \"inertia\". For AP: the damping value (between 0.5, 0.99)\n",
    "\n",
    "result_df = cluster_df(df, CLUSTERING_METHOD, CLUSTERING_METRIC)\n",
    "result_df.set_index('identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = []\n",
    "for i, row in tqdm(df_gold.iterrows(), total=df_gold.shape[0], position=0, leave=True):\n",
    "    guess.append(int(\n",
    "        result_df.loc[row[\"identifier1\"], 'sense'] != result_df.loc[row[\"identifier2\"], 'sense']\n",
    "    #    (1 - avg(cosine_similarity(emb1, cent1)[0][0], cosine_similarity(emb2, cent2)[0][0])) > 3*res1['intra_deviation']\n",
    "    ))\n",
    "\n",
    "df_gold['guess_bin'] = guess\n",
    "F1[\"AP\"] = f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')\n",
    "F1[\"AP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:02<00:00, 48.23it/s]\n"
     ]
    }
   ],
   "source": [
    "CLUSTERING_METHOD = \"KM\" # \"KM\" or \"AP\"\n",
    "CLUSTERING_METRIC = \"inertia\" # For KMeans: \"silhouette\" or \"inertia\". For AP: the damping value (between 0.5, 0.99)\n",
    "\n",
    "result_df = cluster_df(df, CLUSTERING_METHOD, CLUSTERING_METRIC)\n",
    "result_df.set_index('identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = []\n",
    "\n",
    "for i, row in tqdm(df_gold.iterrows(), total=df_gold.shape[0], position=0, leave=True):\n",
    "    guess.append(int(\n",
    "        result_df.loc[row[\"identifier1\"], 'sense'] != result_df.loc[row[\"identifier2\"], 'sense']\n",
    "    ))\n",
    "\n",
    "df_gold['guess_bin'] = guess\n",
    "F1[\"KM-in\"] = f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')\n",
    "F1[\"KM-in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:10<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "CLUSTERING_METHOD = \"KM\" # \"KM\" or \"AP\"\n",
    "CLUSTERING_METRIC = \"silhouette\" # For KMeans: \"silhouette\" or \"inertia\". For AP: the damping value (between 0.5, 0.99)\n",
    "\n",
    "result_df = cluster_df(df, CLUSTERING_METHOD, CLUSTERING_METRIC)\n",
    "result_df.set_index('identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = []\n",
    "\n",
    "for i, row in tqdm(df_gold.iterrows(), total=df_gold.shape[0], position=0, leave=True):\n",
    "    # row1, row2 = result_df.loc[row[\"identifier1\"]], result_df.loc[row[\"identifier2\"]]\n",
    "    # emb1, emb2 = row1['embedding'].reshape(1, -1), row2['embedding'].reshape(1, -1)\n",
    "    # ctr1, ctr2 = row1['centroid'].reshape(1, -1), row2['centroid'].reshape(1, -1)\n",
    "    # dev1, dev2 = row1['intra_deviation'], row2['intra_deviation']\n",
    "    # guess.append(int(\n",
    "    #     cosine_similarity(emb1, emb2)[0][0] < avg(dev1, dev2)\n",
    "    # ))\n",
    "    \n",
    "    # if row1['sense'] == row2['sense']:\n",
    "    #     guess.append(0)\n",
    "    # else:\n",
    "    #     if cosine_similarity(emb1, emb2)[0][0] < cosine_similarity(ctr1, ctr2)[0][0]:\n",
    "    #         guess.append(0)\n",
    "    #     else:\n",
    "    #         guess.append(1)\n",
    "    \n",
    "    guess.append(int(\n",
    "        result_df.loc[row[\"identifier1\"], 'sense'] != result_df.loc[row[\"identifier2\"], 'sense']\n",
    "    ))\n",
    "\n",
    "df_gold['guess_bin'] = guess\n",
    "F1[\"KM-silh\"] = f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')\n",
    "F1[\"KM-silh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CD between centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 62624/62624 [00:32<00:00, 1950.11it/s]\n"
     ]
    }
   ],
   "source": [
    "cdl = []\n",
    "prtl = []\n",
    "\n",
    "for i, row in tqdm(df_gold.iterrows(), total=df_gold.shape[0], position=0, leave=True):\n",
    "    emb1 = result_df.loc[row[\"identifier1\"], \"centroid\"].reshape(1, -1)\n",
    "    emb2 = result_df.loc[row[\"identifier2\"], \"centroid\"].reshape(1, -1)\n",
    "    cs = cosine_similarity(emb1, emb2)[0][0]\n",
    "    cdl.append(1 - cs)\n",
    "    prtl.append(1 / cs)\n",
    "\n",
    "cd_mean = sum(cdl)/len(cdl)\n",
    "prt_mean = sum(prtl)/len(prtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6118155526748179"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gold[\"guess_bin\"] = [int(cd > cd_mean) for cd in cdl]\n",
    "f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6261324913429491"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gold[\"guess_bin\"] = [int(prt > prt_mean) for prt in prtl]\n",
    "f1_score(df_gold['judgment_bin'], df_gold['guess_bin'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1[\"AVG\"] = avg(*F1.values())\n",
    "F1[\"AVG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_results_path = \"./f1.tsv\"\n",
    "f1_keys = sorted(F1.keys(), key=lambda x: x != \"AVG\")\n",
    "\n",
    "if not os.path.exists(f1_results_path):\n",
    "    pd.DataFrame(columns=[\"model\"] + f1_keys).to_csv(f1_results_path, sep='\\t', index=False)\n",
    "\n",
    "df = pd.read_csv(f1_results_path, sep='\\t')\n",
    "\n",
    "df.loc[df['model'] == MODEL, f1_keys] = list(F1.values())\n",
    "if MODEL not in df['model'].values:\n",
    "    df = pd.concat([df, pd.DataFrame({\"model\": [MODEL], **F1})], ignore_index=True)\n",
    "\n",
    "df.to_csv(f1_results_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv(\"./f1-prev.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Calculate the mean of selected columns and assign it to a new column\n",
    "f1['AVG-clustering'] = f1[['AP', 'KM-in', 'KM-silh']].mean(axis=1)\n",
    "\n",
    "# Rename the 'AVG' column to 'AVG-all'\n",
    "f1.rename(columns={'AVG': 'AVG-all'}, inplace=True)\n",
    "\n",
    "# Reorder the columns\n",
    "f1 = f1[['model', 'AVG-clustering', 'AVG-all', 'AP', 'KM-in', 'KM-silh', 'CD', 'PRT']]\n",
    "\n",
    "f1.to_csv(\"./f1.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
